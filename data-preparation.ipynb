{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import queue\n",
    "import random\n",
    "import string\n",
    "import logging\n",
    "import pathlib\n",
    "import requests\n",
    "import feedparser\n",
    "import hashlib\n",
    "import pymysql\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser as dtparse\n",
    "from typing import List, Dict, Optional\n",
    "from lxml import etree\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = pathlib.Path(\"...\")\n",
    "\n",
    "TOPICS = [\n",
    "    \"large language model\",\n",
    "    \"transformer\",\n",
    "    \"reinforcement learning\",\n",
    "    \"computer vision\",\n",
    "    \"natural language processing\",\n",
    "    \"multimodal\",\n",
    "    \"diffusion model\",\n",
    "    \"graph neural network\",\n",
    "    \"federated learning\",\n",
    "    \"speech\",\n",
    "]\n",
    "\n",
    "MAX_PER_TOPIC: Optional[int] = 100\n",
    "CATEGORIES = [\"cs.AI\", \"cs.CL\", \"cs.LG\", \"cs.CV\", \"stat.ML\"]\n",
    "MIN_YEAR = 2023\n",
    "PAGE_SIZE = 100\n",
    "REQUEST_SLEEP_SECONDS = 3.0\n",
    "DOWNLOAD_RETRIES = 3\n",
    "DOWNLOAD_TIMEOUT = 60\n",
    "\n",
    "ARXIV_API = \"http://export.arxiv.org/api/query\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"arxiv-ai-crawler/1.0 (+https://arxiv.org; personal academic use)\"\n",
    "}\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sanitize_filename(name: str, max_len: int = 160) -> str:\n",
    "    name = re.sub(r\"[\\\\/*?\\\"<>|:]\", \"_\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    if len(name) > max_len:\n",
    "        name = name[:max_len].rstrip()\n",
    "    return name\n",
    "\n",
    "# Xây dựng chuỗi query cho arXiv API\n",
    "def build_query(term: str) -> str:\n",
    "    term_part = f'all:\"{term}\"'\n",
    "    if CATEGORIES:\n",
    "        cats = \" OR \".join([f'cat:{c}' for c in CATEGORIES])\n",
    "        q = f\"({term_part}) AND ({cats})\"\n",
    "    else:\n",
    "        q = term_part\n",
    "    return q\n",
    "\n",
    "# Trích arXiv ID từ entry.id\n",
    "def parse_arxiv_id(entry) -> str:\n",
    "    raw = entry.get(\"id\", \"\")\n",
    "    m = re.search(r'arxiv\\.org\\/abs\\/([^\\s]+)$', raw)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    for link in entry.get(\"links\", []):\n",
    "        href = link.get(\"href\", \"\")\n",
    "        m2 = re.search(r'arxiv\\.org\\/abs\\/([^\\s]+)$', href)\n",
    "        if m2:\n",
    "            return m2.group(1)\n",
    "    return \"unknown-\" + \"\".join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "\n",
    "# Lấy link PDF\n",
    "def get_pdf_link(entry) -> Optional[str]:\n",
    "    for link in entry.get(\"links\", []):\n",
    "        if link.get(\"type\") == \"application/pdf\":\n",
    "            return link.get(\"href\")\n",
    "        if link.get(\"title\", \"\").lower() == \"pdf\":\n",
    "            return link.get(\"href\")\n",
    "    return None\n",
    "\n",
    "def is_recent_enough(published_str: str, min_year: int) -> bool:\n",
    "    try:\n",
    "        dt = dtparse.parse(published_str)\n",
    "        return dt.year >= min_year\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def fetch_entries_for_topic(term: str, min_year: int, page_size: int, max_per_topic: Optional[int]) -> List[dict]:\n",
    "    start = 0\n",
    "    collected = []\n",
    "    while True:\n",
    "        params = {\n",
    "            \"search_query\": build_query(term),\n",
    "            \"start\": start,\n",
    "            \"max_results\": page_size,\n",
    "            \"sortBy\": \"submittedDate\",\n",
    "            \"sortOrder\": \"descending\",\n",
    "        }\n",
    "        logging.info(f\"[{term}] Querying arXiv start={start}, size={page_size}\")\n",
    "        resp = requests.get(ARXIV_API, params=params, headers=HEADERS, timeout=60)\n",
    "        time.sleep(REQUEST_SLEEP_SECONDS)  # tuân thủ rate-limit\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        feed = feedparser.parse(resp.text)\n",
    "        entries = feed.get(\"entries\", [])\n",
    "        if not entries:\n",
    "            break\n",
    "\n",
    "        filtered = [e for e in entries if is_recent_enough(e.get(\"published\", \"\"), min_year)]\n",
    "        collected.extend(filtered)\n",
    "\n",
    "        logging.info(f\"[{term}] Got {len(entries)} entries, kept {len(filtered)} (>= {min_year})\")\n",
    "\n",
    "        if len(filtered) < len(entries):\n",
    "            logging.info(f\"[{term}] Encountered entries older than {min_year}; stopping pagination.\")\n",
    "            break\n",
    "\n",
    "        start += page_size\n",
    "\n",
    "        if max_per_topic is not None and len(collected) >= max_per_topic:\n",
    "            collected = collected[:max_per_topic]\n",
    "            break\n",
    "\n",
    "    return collected\n",
    "\n",
    "# Tải pdf\n",
    "def download_pdf(url: str, dest_path: str) -> bool:\n",
    "    for attempt in range(1, DOWNLOAD_RETRIES + 1):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, headers=HEADERS, timeout=DOWNLOAD_TIMEOUT) as r:\n",
    "                r.raise_for_status()\n",
    "                total = int(r.headers.get(\"Content-Length\", 0)) or None\n",
    "                tmp_path = dest_path + \".part\"\n",
    "                with open(tmp_path, \"wb\") as f, tqdm(\n",
    "                    total=total,\n",
    "                    unit=\"B\",\n",
    "                    unit_scale=True,\n",
    "                    desc=os.path.basename(dest_path),\n",
    "                    leave=False\n",
    "                ) as pbar:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 64):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            if total:\n",
    "                                pbar.update(len(chunk))\n",
    "                os.replace(tmp_path, dest_path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Lỗi tải ({attempt}/{DOWNLOAD_RETRIES}) {url}: {e}\")\n",
    "            time.sleep(2 * attempt)\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "    meta_log = []\n",
    "\n",
    "    for topic in TOPICS:\n",
    "        entries = fetch_entries_for_topic(\n",
    "            term=topic,\n",
    "            min_year=MIN_YEAR,\n",
    "            page_size=PAGE_SIZE,\n",
    "            max_per_topic=None\n",
    "        )\n",
    "\n",
    "        logging.info(f\"[{topic}] Total kept entries: {len(entries)}\")\n",
    "\n",
    "        for e in entries:\n",
    "            arxiv_id = parse_arxiv_id(e)\n",
    "            title = e.get(\"title\", \"\").replace(\"\\n\", \" \").strip()\n",
    "            published = e.get(\"published\", \"\")\n",
    "            pdf_url = get_pdf_link(e)\n",
    "\n",
    "            safe_title = sanitize_filename(title)\n",
    "            filename = f\"{sanitize_filename(arxiv_id)} - {safe_title}.pdf\"\n",
    "            save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "            if not pdf_url:\n",
    "                logging.info(f\"Bỏ qua (không có PDF): {arxiv_id} | {title}\")\n",
    "                continue\n",
    "\n",
    "            if os.path.exists(save_path):\n",
    "                logging.info(f\"Đã tồn tại: {filename}\")\n",
    "                continue\n",
    "\n",
    "            logging.info(f\"Tải: {arxiv_id} | {title}\")\n",
    "            ok = download_pdf(pdf_url, save_path)\n",
    "            meta_log.append({\n",
    "                \"topic\": topic,\n",
    "                \"arxiv_id\": arxiv_id,\n",
    "                \"title\": title,\n",
    "                \"published\": published,\n",
    "                \"pdf_url\": pdf_url,\n",
    "                \"saved\": save_path if ok else None,\n",
    "                \"status\": \"downloaded\" if ok else \"failed\"\n",
    "            })\n",
    "\n",
    "    # Lưu metadata để tiện tra cứu\n",
    "    # meta_path = os.path.join(OUTPUT_DIR, f\"_arxiv_download_meta_{int(time.time())}.json\")\n",
    "    # with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(meta_log, f, ensure_ascii=False, indent=2)\n",
    "    # logging.info(f\"Đã lưu metadata: {meta_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d857fe",
   "metadata": {},
   "source": [
    "docker run --rm -p 18070:8070 -p 18071:8071 lfoppiano/grobid:latest-crf\n",
    "\n",
    "$inDir  = \"C:\\Users\\NCPC\\OneDrive\\Python\\Search-Engine\\paper-crawl\"\n",
    "$outDir = \"C:\\Users\\NCPC\\OneDrive\\Python\\Search-Engine\\data-crawl\"\n",
    "New-Item -ItemType Directory -Force -Path $outDir | Out-Null\n",
    "\n",
    "Get-ChildItem -Path $inDir -Filter *.pdf | ForEach-Object {\n",
    "    $pdfPath = $_.FullName\n",
    "    $outPath = Join-Path $outDir ($_.BaseName + \".tei.xml\")\n",
    "\n",
    "    & curl.exe -s -S -X POST `\n",
    "        -F \"input=@$pdfPath\" `\n",
    "        -F \"consolidateHeader=1\" `\n",
    "        -F \"consolidateCitations=0\" `\n",
    "        \"http://localhost:18070/api/processFulltextDocument\" `\n",
    "        -o \"$outPath\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b050db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEI_NS = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "tei_dir = pathlib.Path(r\"...\")\n",
    "\n",
    "load_dotenv()  # tự đọc .env\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=DB_HOST, user=DB_USER, password=DB_PASS,\n",
    "    database=DB_NAME, charset=\"utf8mb4\", autocommit=False\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "def h(s: str) -> str:\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_xml_lenient(path):\n",
    "    parser = etree.XMLParser(\n",
    "        ns_clean=True,\n",
    "        remove_blank_text=True,\n",
    "        recover=True,        # cho phép bỏ qua lỗi ID trùng, XML không chuẩn\n",
    "        huge_tree=True,\n",
    "        resolve_entities=False\n",
    "    )\n",
    "    with open(path, \"rb\") as f:\n",
    "        try:\n",
    "            return etree.parse(f, parser=parser)\n",
    "        except etree.XMLSyntaxError as e:\n",
    "            print(f\"[WARN] Không parse được {path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def parse_tei(tei_path: pathlib.Path):\n",
    "    root = load_xml_lenient(str(tei_path))\n",
    "    if root is None:\n",
    "        return \"\", \"\", \"\", \"\", []   # bỏ qua file lỗi\n",
    "\n",
    "    title = root.xpath(\"string(//tei:titleStmt/tei:title)\", namespaces=TEI_NS).strip()\n",
    "    authors = [\" \".join(a.itertext()).strip()\n",
    "               for a in root.xpath(\"//tei:author/tei:persName\", namespaces=TEI_NS)]\n",
    "    year = (root.xpath(\"string(//tei:sourceDesc//tei:date/@when)\", namespaces=TEI_NS)\n",
    "            or root.xpath(\"string(//tei:sourceDesc//tei:date)\", namespaces=TEI_NS) or \"\").strip()\n",
    "    abstract = root.xpath(\"string(//tei:profileDesc/tei:abstract)\", namespaces=TEI_NS).strip()\n",
    "\n",
    "    paras = []\n",
    "    for p in root.xpath(\"//tei:text//tei:body//tei:p\", namespaces=TEI_NS):\n",
    "        txt = \" \".join(p.itertext()).strip()\n",
    "        if len(txt) < 40:\n",
    "            continue\n",
    "        sec = p.xpath(\"string(ancestor::tei:div[1]/@type)\", namespaces=TEI_NS) or \"\"\n",
    "        paras.append((sec, None, txt))\n",
    "    return title, \"; \".join(authors), year, abstract, paras\n",
    "\n",
    "def upsert_doc(doc_id, pdf_path, tei_path, title, authors, year, abstract):\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO docs (doc_id, pdf_path, tei_path, title, authors, year, abstract)\n",
    "        VALUES (%s,%s,%s,%s,%s,%s,%s)\n",
    "        ON DUPLICATE KEY UPDATE\n",
    "            pdf_path=VALUES(pdf_path), tei_path=VALUES(tei_path),\n",
    "            title=VALUES(title), authors=VALUES(authors),\n",
    "            year=VALUES(year), abstract=VALUES(abstract)\n",
    "    \"\"\", (doc_id, str(pdf_path), str(tei_path), title, authors, year, abstract))\n",
    "\n",
    "def upsert_chunk(chunk_id, doc_id, section, page, text):\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO chunks (chunk_id, doc_id, section, page, text)\n",
    "        VALUES (%s,%s,%s,%s,%s)\n",
    "        ON DUPLICATE KEY UPDATE\n",
    "            section=VALUES(section), page=VALUES(page), text=VALUES(text)\n",
    "    \"\"\", (chunk_id, doc_id, section, page, text))\n",
    "\n",
    "# map TEI -> PDF path theo tên file (đổi nếu bạn có mapping khác)\n",
    "pdf_dir = pathlib.Path(r\"...\")\n",
    "\n",
    "for tei_path in tqdm(list(tei_dir.glob(\"*.tei.xml\"))):\n",
    "    stem = tei_path.stem.replace(\".fulltext\", \"\")\n",
    "    pdf_path = pdf_dir / (stem + \".pdf\")\n",
    "    doc_id = h(str(pdf_path))\n",
    "\n",
    "    title, authors, year, abstract, paras = parse_tei(tei_path)\n",
    "    upsert_doc(doc_id, pdf_path, tei_path, title, authors, year, abstract)\n",
    "\n",
    "    for i, (sec, page, txt) in enumerate(paras):\n",
    "        chunk_id = f\"{doc_id}_{i}\"\n",
    "        upsert_chunk(chunk_id, doc_id, sec, page, txt)\n",
    "\n",
    "conn.commit()\n",
    "cur.close(); conn.close()\n",
    "print(\"Done TEI→MySQL.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
